{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_output = utils.loadTrainingData()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Input Dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input\n",
    "# train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**capuchon_insertion**'s standard deviation is quite low ($0.024425$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_input.hist(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_input.boxplot(column=train_input.columns.tolist()[1:], figsize=(20,10))\n",
    "\n",
    "for column in train_input.columns.tolist()[1:]:\n",
    "    plt.figure()\n",
    "    train_input.boxplot([column])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.matshow(train_input.corr())\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_input)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**capuchon_insertion** might not be a relevent parameter, more than 50% of the population is na ($\\frac{18627}{34515} \\approx 0.53967840069$).  \n",
    "But nan values could also be filed with the average value as shown below...  \n",
    "If every/most defective individual is set to na then we can eliminate this feature.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = train_input[\"capuchon_insertion\"].mean()\n",
    "capuchon_insertion_no_nan = train_input[\"capuchon_insertion\"].fillna(mean, inplace=False)\n",
    "capuchon_insertion_no_nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_defect = train_input.copy() # Deep copy\n",
    "defect_index = train_output.index[train_output[\"result\"] == 1].tolist()\n",
    "train_input_defect = train_input_defect.iloc[defect_index,:]\n",
    "train_input_defect.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among the 305 defective individuals, 110 of them do not have a **capuchon_insertion** value ($\\frac{110}{305} \\approx 0.3606$).  \n",
    "Most of the defective individuals have a **capuchon_insertion** value, maybe it is worth keeping it...  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unscaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for PCA\n",
    "train_input_pca = train_input.copy() # Deep copy\n",
    "train_input_pca = train_input_pca[train_input_pca.columns[~train_input_pca.columns.isin([\"id\", \"capuchon_insertion\"])]]\n",
    "\n",
    "# PCA\n",
    "pca = PCA()\n",
    "pca.fit(train_input_pca)\n",
    "\n",
    "pca.explained_variance_ratio_\n",
    "plt.xticks(range(12))\n",
    "plt.plot(range(12), pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "plt.xticks(range(12))\n",
    "plt.plot(range(12), cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=12).fit(train_input_pca)\n",
    "X_pca = pca_model.transform(train_input_pca)\n",
    "\n",
    "# Number of components\n",
    "nb_comp = pca_model.components_.shape[0]\n",
    "\n",
    "# Index of the most important feature on EACH component i.e. largest absolute value\n",
    "most_important = [np.abs(pca_model.components_[i]).argmax() for i in range(nb_comp)]\n",
    "\n",
    "# Features names\n",
    "initial_feature_names = pca_model.feature_names_in_\n",
    "\n",
    "# Name of the most important feature on EACH component\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(nb_comp)]\n",
    "\n",
    "dic = {'PC{}'.format(i+1) : most_important_names[i] for i in range(nb_comp)}\n",
    "\n",
    "# build the dataframe\n",
    "# df = pd.DataFrame(sorted(dic.items()))\n",
    "\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3D figure\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Plot with color\n",
    "colors = {1: 'red', 0: 'green'}\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], X_pca[:, 2], c=train_output[\"result\"].map(colors))\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Dim 1')\n",
    "ax.set_ylabel('Dim 2')\n",
    "ax.set_zlabel('Dim 3')\n",
    "\n",
    "# Show figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D figure\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Plot with color and transparency\n",
    "colors = {1: 'red', 0: 'green'}\n",
    "alphas = {1: 1, 0: 0.1}\n",
    "ax.scatter(X_pca[:, 0], X_pca[:, 1], c=train_output[\"result\"].map(colors), alpha=train_output[\"result\"].map(alphas))\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Dim 1')\n",
    "ax.set_ylabel('Dim 2')\n",
    "\n",
    "# Show figure\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for PCA\n",
    "train_input_pca_scale = train_input.copy() # Deep copy\n",
    "train_input_pca_scale = train_input_pca_scale[train_input_pca_scale.columns[~train_input_pca_scale.columns.isin([\"id\", \"capuchon_insertion\"])]]\n",
    "\n",
    "# Scale data\n",
    "standard_scaler = StandardScaler(copy=False)\n",
    "standard_scaler.fit_transform(train_input_pca_scale)\n",
    "\n",
    "# PCA\n",
    "scaled_pca = PCA()\n",
    "scaled_pca.fit(train_input_pca_scale)\n",
    "\n",
    "scaled_pca.explained_variance_ratio_\n",
    "plt.xticks(range(12))\n",
    "plt.plot(range(12), scaled_pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_cumsum = np.cumsum(scaled_pca.explained_variance_ratio_)\n",
    "plt.xticks(range(12))\n",
    "plt.plot(range(12), scaled_cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_pca_model = PCA(n_components=12).fit(train_input_pca_scale)\n",
    "X_scaled_pca = scaled_pca_model.transform(train_input_pca_scale)\n",
    "\n",
    "# Number of components\n",
    "nb_comp = scaled_pca_model.components_.shape[0]\n",
    "\n",
    "# Index of the most important feature on EACH component i.e. largest absolute value\n",
    "most_important = [np.abs(scaled_pca_model.components_[i]).argmax() for i in range(nb_comp)]\n",
    "\n",
    "# Features names\n",
    "initial_feature_names = scaled_pca_model.feature_names_in_\n",
    "\n",
    "# Name of the most important feature on EACH component\n",
    "most_important_names = [initial_feature_names[most_important[i]] for i in range(nb_comp)]\n",
    "\n",
    "scaled_dic = {'PC{}'.format(i+1) : most_important_names[i] for i in range(nb_comp)}\n",
    "\n",
    "# build the dataframe\n",
    "# df = pd.DataFrame(sorted(dic.items()))\n",
    "\n",
    "scaled_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_scaled_pca[:, :3]\n",
    "\n",
    "# Create 3D figure\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "# Plot with color\n",
    "colors = {1:'red', 0:'green'}\n",
    "ax.scatter(X_scaled_pca[:, 0], X_scaled_pca[:, 1], X_scaled_pca[:, 2], c=train_output[\"result\"].map(colors))\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Dim 1')\n",
    "ax.set_ylabel('Dim 2')\n",
    "ax.set_zlabel('Dim 3')\n",
    "\n",
    "# Show figure\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2D figure\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "# Plot with color and transparency\n",
    "colors = {1: 'red', 0: 'green'}\n",
    "alphas = {1: 1, 0: 0.1}\n",
    "ax.scatter(X_scaled_pca[:, 0], X_scaled_pca[:, 1], c=train_output[\"result\"].map(colors), alpha=train_output[\"result\"].map(alphas))\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Dim 1')\n",
    "ax.set_ylabel('Dim 2')\n",
    "\n",
    "# Show figure\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ = train_input[train_input.columns[~train_input.columns.isin([\"id\", \"capuchon_insertion\"])]]\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_input_, train_output[\"result\"], test_size = 0.3, random_state = 123)\n",
    "\n",
    "# Scale data (MLP is very sensitive to scaling)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Scale data (MLP is very sensitive to scaling)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Copy dataset\n",
    "train_input_remove = train_input.copy()\n",
    "train_output_remove = train_output.copy()\n",
    "\n",
    "# Scale data (MLP is very sensitive to scaling and allow for a unique treshold)\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(X_train)\n",
    "# X_train = scaler.transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# Find outliers\n",
    "threshold = 3\n",
    "outliers = np.array([], dtype = int)\n",
    "for col_name in list(input_header.values())[1:]:\n",
    "    z = np.abs(stats.zscore(train_input[col_name]))\n",
    "    outliers = np.append(outliers, np.where(z > threshold))\n",
    "outliers_no_duplicate = np.array([], dtype=int)\n",
    "for i in range(np.size(outliers)):\n",
    "    if(outliers[i] not in outliers_no_duplicate):\n",
    "        outliers_no_duplicate = np.append(outliers_no_duplicate, outliers[i])\n",
    "np.size(outliers_no_duplicate)\n",
    "\n",
    "# Randomly remove some valid individuals\n",
    "train_input_remove = train_input_remove.iloc[~outliers_no_duplicate,:]\n",
    "\n",
    "# Create new datasets\n",
    "train_input_remove = train_input_remove[train_input_remove.columns[~train_input_remove.columns.isin([\"id\", \"capuchon_insertion\"])]]\n",
    "\n",
    "for column in train_input_remove.columns.tolist()[1:]:\n",
    "    plt.figure()\n",
    "    train_input_remove.boxplot([column])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Output Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output\n",
    "# train_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_output_bool = train_output.copy() # Deep copy\n",
    "# train_output_bool[\"result\"] = train_output_bool[\"result\"].astype(bool)\n",
    "# train_output_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output[\"result\"].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unbalanced result classes. Defect class is under-represented in the population ($\\frac{305}{34515} \\approx 0.008836737650296972$)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "60e99164659eb01783c8f561e8a597a048720cf8240e4ce5ecfbb52f98d8d756"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
